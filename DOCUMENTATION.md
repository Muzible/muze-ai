# Muzible Muze AI - Technical Documentation v2

> **Text-to-Music Generation with Latent Diffusion & Voice Conditioning**

---

## Table of Contents

1. [System Architecture](#system-architecture)
2. [V2 Architecture - Voice Stream Attention](#v2-architecture---voice-stream-attention)
3. [Training Pipeline](#training-pipeline)
4. [Dataset Format](#dataset-format)
5. [Conditioning System](#conditioning-system)
6. [File Structure](#file-structure)
7. [Usage Scenarios](#usage-scenarios)
8. [Inference - Music Generation](#inference---music-generation)
9. [Detailed File Descriptions](#detailed-file-descriptions)
10. [FAQ & Troubleshooting](#faq--troubleshooting)
11. [Model Size Configuration](#model-size-configuration)
12. [Requirements](#requirements)

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MUZIBLE MUZE AI v2                                  â”‚
â”‚                   Text-to-Music Generation Pipeline                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   INPUTS     â”‚    â”‚   ENCODERS   â”‚    â”‚   OUTPUTS    â”‚                  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚  â”‚ Text Prompt  â”‚â”€â”€â”€â–¶â”‚ T5/CLAP      â”‚â”€â”€â”€â–¶â”‚              â”‚                  â”‚
â”‚  â”‚ Voice Sample â”‚â”€â”€â”€â–¶â”‚ Resemblyzer  â”‚â”€â”€â”€â–¶â”‚  UNet V2     â”‚                  â”‚
â”‚  â”‚ Style Ref    â”‚â”€â”€â”€â–¶â”‚ ECAPA-TDNN   â”‚â”€â”€â”€â–¶â”‚  (Diffusion) â”‚â”€â”€â–¶ Audio WAV    â”‚
â”‚  â”‚ Lyrics       â”‚â”€â”€â”€â–¶â”‚ Gruut/eSpeak â”‚â”€â”€â”€â–¶â”‚              â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    CORE COMPONENTS                                    â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  AudioVAE (224M)  â”‚  UNet V2 (722M-6.1B)  â”‚  HiFi-GAN Vocoder       â”‚  â”‚
â”‚  â”‚  - Mel â†’ Latent   â”‚  - Noise â†’ Latent     â”‚  - Mel â†’ Waveform       â”‚  â”‚
â”‚  â”‚  - Latent â†’ Mel   â”‚  - Voice Attention    â”‚  - 32kHz output         â”‚  â”‚
â”‚  â”‚  - KL + STFT Loss â”‚  - Section Cond.      â”‚  - High quality         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Main Models

| Component | Parameters | Function |
|-----------|------------|----------|
| **AudioVAE** | 55-889M | Audio compression to latent space |
| **UNet V2** | 722M-6.1B | Latent diffusion denoising |
| **T5 Encoder** | 250M | Text prompt encoding |
| **CLAP** | 600M | Audio-text joint embeddings |
| **HiFi-GAN** | ~13M | High-quality vocoder (32kHz) |

---

## V2 Architecture - Voice Stream Attention

### What is VoiceStreamAttention?

**VoiceStreamAttention** is a **dedicated cross-attention mechanism** that allows the diffusion model to attend to voice embedding **separately** from text embedding.

```
Standard Cross-Attention (v1):
    Q = latent, K,V = text_embedding
    
V2 Voice Stream Attention:
    Branch 1: Q = latent, K,V = text_embedding      â†’ text_attn
    Branch 2: Q = latent, K,V = voice_embedding     â†’ voice_attn
    Output: gate * voice_attn + (1-gate) * text_attn
```

### Why is it important?

1. **Voice quality** - Model can "focus" on voice characteristics independently
2. **Timbre control** - Voice gate allows dynamic balance between text and voice
3. **Better disentanglement** - Voice separated from semantics

### V2 Architecture Diagram

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              UNet V2 Block                   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Input Latent â”€â”€â”€â”€â”€â”€â–¶â”‚  ResBlock  â”‚  Self-Attn  â”‚  Cross-Attn     â”‚
    [B,128,H,W]     â”‚            â”‚             â”‚                  â”‚
                    â”‚            â”‚             â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚            â”‚             â”‚   â”‚ Text K,V   â”‚ â”‚
                    â”‚            â”‚             â”‚   â”‚ [B,768]    â”‚ â”‚
                    â”‚            â”‚             â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚            â”‚             â”‚         â”‚        â”‚
                    â”‚            â”‚             â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚            â”‚             â”‚   â”‚ text_attn  â”‚ â”‚
                    â”‚            â”‚             â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚            â”‚             â”‚         â”‚        â”‚
                    â”‚            â”‚             â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚            â”‚             â”‚   â”‚ GATED MIX  â”‚â—€â”€â”€ gate (learnable)
                    â”‚            â”‚             â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚            â”‚             â”‚         â”‚        â”‚
                    â”‚            â”‚             â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚            â”‚             â”‚   â”‚ voice_attn â”‚ â”‚
                    â”‚            â”‚             â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚            â”‚             â”‚         â”‚        â”‚
                    â”‚            â”‚             â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚            â”‚             â”‚   â”‚ Voice K,V  â”‚ â”‚
                    â”‚            â”‚             â”‚   â”‚ [B,256]    â”‚ â”‚
                    â”‚            â”‚             â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                              Output Latent [B,128,H,W]
```

### VoiceEmbeddingFusion (v2)

In v2, we use **two voice embeddings**:

| Embedding | Dimension | Model | Characteristics |
|-----------|-----------|-------|-----------------|
| **Resemblyzer** | 256 | GE2E | General speaker verification |
| **ECAPA-TDNN** | 192 | SpeechBrain | Better for singing voice |

```python
# Fusion
voice_fused = VoiceEmbeddingFusion(
    resemblyzer_embed,  # [B, 256]
    ecapa_embed         # [B, 192]
)
# Output: [B, 256] - weighted projection
```

---

## Training Pipeline

### Phase Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRAINING PIPELINE v2                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Phase 1: VAE (Audio Compression)                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  Audio WAV â†’ Mel Spectrogram â†’ Encoder â†’ Î¼, Ïƒ â†’ z (latent) â†’ Decoder â†’ Mel â”‚
â”‚                                                                            â”‚
â”‚  Loss: MSE(mel, mel_recon) + Î²*KL(z) + STFT_loss                          â”‚
â”‚  Target: Reconstruct audio with minimal latent dim (128)                   â”‚
â”‚                                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Phase 2: Composition Planner (Optional)                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  Track features â†’ MLP â†’ Section plan (verse, chorus, bridge, etc.)         â”‚
â”‚                                                                            â”‚
â”‚  Loss: CrossEntropy(predicted_sections, ground_truth_sections)             â”‚
â”‚  Target: Learn song structure from metadata                                â”‚
â”‚                                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Phase 3: Latent Diffusion Model (LDM)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚  Noise z_T â†’ UNet V2 (conditioned) â†’ ... â†’ z_0 â†’ VAE Decode â†’ Audio        â”‚
â”‚                                                                            â”‚
â”‚  Conditioning:                                                             â”‚
â”‚  - Text: T5/CLAP embedding [768]                                           â”‚
â”‚  - Voice: Resemblyzer [256] + ECAPA [192]                                  â”‚
â”‚  - Section: type, position, energy, tempo, key                             â”‚
â”‚  - Audio: CLAP audio embedding [512]                                       â”‚
â”‚  - Beat/Chord/Phoneme encoders                                             â”‚
â”‚                                                                            â”‚
â”‚  Loss: MSE(predicted_noise, actual_noise) + cfg_loss                       â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LDM Training with All Conditioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LDM v2 TRAINING - FULL CONDITIONING                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  INPUTS (per batch):                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ audio_path: "music/fma_small/000/000123.mp3"                         â”‚  â”‚
â”‚  â”‚ prompt: "Energetic rock with electric guitar and drums"              â”‚  â”‚
â”‚  â”‚ section_type: "chorus"                                               â”‚  â”‚
â”‚  â”‚ position: 0.35                                                       â”‚  â”‚
â”‚  â”‚ energy: 0.82                                                         â”‚  â”‚
â”‚  â”‚ tempo: 128.0                                                         â”‚  â”‚
â”‚  â”‚ key: "C major"                                                       â”‚  â”‚
â”‚  â”‚ voice_embedding: [256-dim tensor]                                    â”‚  â”‚
â”‚  â”‚ ecapa_embedding: [192-dim tensor]                                    â”‚  â”‚
â”‚  â”‚ clap_audio_embedding: [512-dim tensor]                               â”‚  â”‚
â”‚  â”‚ clap_text_embedding: [512-dim tensor]                                â”‚  â”‚
â”‚  â”‚ num_beats: 64                                                        â”‚  â”‚
â”‚  â”‚ beat_positions: [[0.0, 0.47], [0.47, 0.94], ...]                     â”‚  â”‚
â”‚  â”‚ current_chord: "C:maj"                                               â”‚  â”‚
â”‚  â”‚ phonemes_ipa: "Ã°Éªs Éªz É™ tÉ›st"                                        â”‚  â”‚
â”‚  â”‚ f0_contour: [440.0, 442.1, ...]                                      â”‚  â”‚
â”‚  â”‚ vibrato_rate, vibrato_depth, breath_positions, ...                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â”‚  PROCESSING:                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  1. Load audio â†’ Mel spectrogram                                     â”‚  â”‚
â”‚  â”‚  2. VAE.encode(mel) â†’ z_0 (latent)                                   â”‚  â”‚
â”‚  â”‚  3. Sample timestep t ~ Uniform(0, T)                                â”‚  â”‚
â”‚  â”‚  4. Add noise: z_t = âˆšá¾±â‚œÂ·z_0 + âˆš(1-á¾±â‚œ)Â·Îµ                            â”‚  â”‚
â”‚  â”‚  5. Encode conditioning:                                             â”‚  â”‚
â”‚  â”‚     - text_embed = T5(prompt)           [768]                        â”‚  â”‚
â”‚  â”‚     - voice_fused = Fusion(voice, ecapa) [256]                       â”‚  â”‚
â”‚  â”‚     - section_cond = SectionModule(...)  [1024]                      â”‚  â”‚
â”‚  â”‚  6. UNet forward: Îµ_Î¸ = UNet(z_t, t, text_embed, voice_fused, ...)  â”‚  â”‚
â”‚  â”‚  7. Loss = MSE(Îµ_Î¸, Îµ)                                               â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dataset Format

### Dataset JSON Structure (v3.1)

```json
{
  "audio_path": "music/fma_small/000/000123.mp3",
  "track_id": 123,
  "artist": "Artist Name",
  "title": "Song Title",
  "album": "Album Name",
  "genre": "rock",
  "year": 2023,
  
  "prompt": "Energetic rock song with electric guitar riffs and powerful drums...",
  "text_sentiment": "positive",
  
  "duration": 180.5,
  "sample_rate": 32000,
  "tempo": 128.0,
  "key": "C major",
  "time_signature": "4/4",
  "loudness_db": -8.5,
  "energy": 0.82,
  
  "has_vocals": true,
  "lyrics": "Transcribed lyrics from the song...",
  "phonemes_ipa": "Ã°Éªs Éªz Ã°É™ faÉªnÉ™l transkrÉªpÊƒÉ™n",
  
  "voice_embedding": [0.12, -0.34, ...],
  "voice_embedding_separated": [0.15, -0.31, ...],
  
  "clap_audio_embedding": [0.05, 0.12, ...],
  "clap_text_embedding": [0.08, 0.15, ...],
  
  "segments": [
    {
      "type": "intro",
      "start": 0.0,
      "end": 15.2,
      "energy": 0.3,
      "has_vocals": false
    },
    {
      "type": "verse",
      "start": 15.2,
      "end": 45.8,
      "energy": 0.6,
      "has_vocals": true,
      "lyrics": "First verse lyrics..."
    },
    {
      "type": "chorus",
      "start": 45.8,
      "end": 76.4,
      "energy": 0.9,
      "has_vocals": true,
      "lyrics": "Chorus lyrics..."
    }
  ],
  
  "beat_positions": [[0.0, 0.47], [0.47, 0.94], ...],
  "downbeat_positions": [0.0, 1.88, 3.76, ...],
  "chord_progression": ["C:maj", "G:maj", "Am:min", "F:maj"],
  
  "f0_contour": [440.0, 442.1, 438.5, ...],
  "f0_voiced_mask": [true, true, false, ...],
  "vibrato_rate": 5.2,
  "vibrato_depth": 0.15,
  "vibrato_extent": 0.8,
  "breath_positions": [[12.5, 12.8], [25.1, 25.4], ...],
  "phoneme_timestamps": [
    {"phoneme": "Ã°", "start": 0.0, "end": 0.05},
    {"phoneme": "Éª", "start": 0.05, "end": 0.12}
  ]
}
```

---

## Conditioning System

### Conditioning Summary

| Parameter | Type | Dimension | Encoder |
|-----------|------|-----------|---------|
| `prompt` | str | â†’ 768 | T5TextEncoder |
| `section_type` | str | â†’ 128 | SectionEmbedding |
| `position` | float 0-1 | â†’ 128 | SinusoidalPosEmb |
| `energy` | float 0-1 | â†’ 64 | Linear |
| `tempo` | float BPM | â†’ 64 | Linear (normalized) |
| `key` | int 0-23 | â†’ 64 | KeyEmbedding |
| `loudness` | float dB | â†’ 64 | Linear |
| `has_vocals` | bool | â†’ 32 | Linear |
| `sentiment` | str | â†’ 64 | SentimentEmbedding |
| `genre` | str | â†’ 64 | GenreEmbedding |
| `artist` | str | â†’ 64 | ArtistEmbedding |
| `clap_audio` | 512-dim | â†’ 128 | Linear projection |
| `clap_text` | 512-dim | â†’ 128 | Linear projection |
| `voice_embedding` | 256-dim | â†’ 256 | VoiceStreamAttention |
| `ecapa_embedding` | 192-dim | â†’ 256 | VoiceEmbeddingFusion |
| `num_beats` | int | â†’ 64 | BeatEmbedding |
| `beat_positions` | List[List[float]] | â†’ 64 | BeatEmbedding |
| `time_signature` | str | â†’ 32 | TimeSignatureEmb |
| `current_chord` | str | â†’ 64 | ChordEmbedding |
| `phonemes_ipa` | str | â†’ 128 | PhonemeEncoder (GRU) |
| `f0_contour` | List[float] | â†’ 64 | F0Encoder (Conv1d) |
| `f0_voiced_mask` | List[bool] | â†’ 32 | VoicedMaskEncoder |
| `vibrato_rate` | float Hz | â†’ 64 | VibratoEncoder |
| `vibrato_depth` | float cents | â†’ 64 | VibratoEncoder |
| `vibrato_extent` | float 0-1 | â†’ 64 | VibratoEncoder |
| `breath_positions` | List[List[float]] | â†’ 32 | BreathEncoder |

### Fusion Dimensions

```
Base:     section(128) + position(128) + energy(64) + tempo(64) + key(64) + text(512)
          + loudness(64) + has_vocals(32) + sentiment(64) + genre(64) + artist(64) = 1248

Optional: + clap(128) + beat(64) + chord(64) + phoneme(128)
          + pitch(64) + vibrato(64) + breath(32) + phoneme_ts(64) = 1856

Final:    Fusion MLP â†’ output_dim (1024)
```

---

## Inference Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INFERENCE PIPELINE v2                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  INPUT: "Energetic rock song with female vocals"                           â”‚
â”‚         + voice_sample.wav (optional)                                      â”‚
â”‚         + lyrics (optional)                                                â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 1: Text Encoding                                               â”‚   â”‚
â”‚  â”‚   prompt â†’ T5Encoder â†’ text_embed [768]                             â”‚   â”‚
â”‚  â”‚   prompt â†’ CLAPTextEncoder â†’ clap_text_embed [512]                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                                â”‚
â”‚                           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 2: Voice Encoding (if voice_sample provided)                   â”‚   â”‚
â”‚  â”‚   voice.wav â†’ Resemblyzer â†’ voice_embed [256]                       â”‚   â”‚
â”‚  â”‚   voice.wav â†’ ECAPA-TDNN â†’ ecapa_embed [192]                        â”‚   â”‚
â”‚  â”‚   Fusion(voice, ecapa) â†’ voice_fused [256]                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                                â”‚
â”‚                           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 3: Composition Planning                                        â”‚   â”‚
â”‚  â”‚   Template "verse_chorus" â†’ [intro, verse, chorus, verse, chorus]   â”‚   â”‚
â”‚  â”‚   Each section: duration, energy, position                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                                â”‚
â”‚                           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 4: Per-Section Generation (DDPM/DDIM)                          â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚   For each section:                                                 â”‚   â”‚
â”‚  â”‚     z_T ~ N(0, I)                     # Start with noise            â”‚   â”‚
â”‚  â”‚     for t = T, T-1, ..., 1:                                         â”‚   â”‚
â”‚  â”‚       Îµ_Î¸ = UNet(z_t, t, text_embed, voice_fused, section_cond)    â”‚   â”‚
â”‚  â”‚       z_{t-1} = DDPM_step(z_t, Îµ_Î¸, t)                             â”‚   â”‚
â”‚  â”‚     z_0 = final denoised latent                                     â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                                â”‚
â”‚                           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 5: Audio Decoding                                              â”‚   â”‚
â”‚  â”‚   z_0 â†’ VAE.decode() â†’ mel_spectrogram [128, T]                     â”‚   â”‚
â”‚  â”‚   mel â†’ HiFi-GAN â†’ audio_waveform [samples]                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                                â”‚
â”‚                           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 6: Concatenation                                               â”‚   â”‚
â”‚  â”‚   [intro_audio, verse_audio, chorus_audio, ...] â†’ final_audio.wav   â”‚   â”‚
â”‚  â”‚   Apply crossfade between sections (50ms)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  OUTPUT: final_audio.wav (44.1kHz stereo)                                  â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DDPM vs DDIM

| Method | Steps | Speed | Quality |
|--------|-------|-------|---------|
| DDPM | 1000 | Slow (~2min/30s) | Best |
| DDPM-50 | 50 | Medium (~15s/30s) | Good |
| DDIM-50 | 50 | Medium (~15s/30s) | Good |
| DDIM-20 | 20 | Fast (~6s/30s) | Acceptable |

**Recommendation:** Use DDIM-50 for production, DDPM-1000 for final renders.

---

## UNet V2 - Key Modules

```python
UNetV2(
    in_channels=128,        # latent_dim from VAE (v2: increased from 8)
    out_channels=128,
    model_channels=320,     # main "size knob"
    num_res_blocks=2,
    attention_resolutions=[8, 4, 2],
    context_dim=768,        # text embedding dim
    num_heads=8,
    
    # v2: Voice conditioning
    voice_dim=256,              # Resemblyzer embedding
    ecapa_dim=192,              # ECAPA-TDNN embedding (voice_emb_separated)
    clap_dim=512,               # CLAP audio+text embedding
    use_voice_stream=True,      # VoiceStreamAttention
    use_dual_voice=True,        # Resemblyzer + ECAPA fusion
    
    # v2: Beat/Chord/Phoneme
    use_clap=True,
    use_beat=True,              # BeatEmbedding
    use_chord=True,             # ChordEmbedding
    use_phonemes=True,          # PhonemeEncoder
    
    # v3: Pitch conditioning
    use_pitch=True,
    f0_encoder_dim=64,
    
    # v3.1: Singing expression
    vibrato_encoder_dim=64,
    breath_encoder_dim=32,
    phoneme_timestamp_encoder_dim=64,
    
    # Performance
    use_gradient_checkpointing=True,
)
```

### SectionConditioningModule - Sections and Metadata

```python
SectionConditioningModule(
    output_dim=1024,
    text_embed_dim=768,
    section_embed_dim=128,
    num_keys=24,            # C-B major/minor
    
    # v2 modules:
    use_clap=True,
    use_beat=True,
    use_chord=True,
    use_phonemes=True,
    
    # v3 modules:
    use_pitch=True,
    clap_dim=512,
    voice_dim=256,
)

# Forward accepts 30+ conditioning parameters
section_cond.forward(
    text_embed,                     # [B, 768] or [B, seq, 768]
    section_type,                   # List[str]
    position, energy, tempo,        # [B] floats
    key_idx,                        # [B] int 0-23
    loudness, has_vocals,           # [B] v3 metadata
    sentiment_score, genres, artists,
    clap_audio_embedding, clap_text_embedding,
    num_beats, beat_positions, time_signature, current_chord,
    phonemes_ipa, voice_embedding,
    f0, f0_coarse, f0_voiced_mask,
    vibrato_rate, vibrato_depth, vibrato_extent,
    breath_positions, phoneme_timestamps,
    segment_duration,
)
â†’ (conditioning [B, 1024], phoneme_durations or None)
```

---

## File Structure

```
muzible-muze-ai/
â”œâ”€â”€ ğŸ“„ train_v2.py                 # Training script v2 (3-phase)
â”œâ”€â”€ ğŸ“„ inference_v2.py             # Music generation from model
â”œâ”€â”€ ğŸ“„ build_dataset_v2.py         # Dataset builder v2 (full extraction)
â”‚
â”œâ”€â”€ ğŸ“ docs_v2/                    # Documentation
â”‚   â””â”€â”€ ğŸ“„ DATASET_BUILDER.md      # Full dataset builder documentation
â”‚
â”œâ”€â”€ ğŸ“ tools/
â”‚   â”œâ”€â”€ ğŸ“„ f0_extractor.py         # F0/pitch extraction
â”‚   â””â”€â”€ ğŸ“„ analyze_metadata.py     # Metadata analysis
â”‚
â”œâ”€â”€ ğŸ“ tools_v2/                   # Tools v2
â”‚   â”œâ”€â”€ ğŸ“„ segment_annotator.py    # Segment detection (verse/chorus)
â”‚   â”œâ”€â”€ ğŸ“„ generate_artist_embeddings.py  # Voice embeddings generation
â”‚   â””â”€â”€ ğŸ“„ scan_mp3_folder.py      # MP3 folder scanning
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ ğŸ“„ audio_vae.py            # Audio VAE (audio â†’ latent compression)
â”‚   â”œâ”€â”€ ğŸ“„ vocoder.py              # Vocoder (mel â†’ waveform)
â”‚   â””â”€â”€ ğŸ“„ voice_synthesis.py      # Voice cloning & singing (GPT-SoVITS, XTTS)
â”‚
â”œâ”€â”€ ğŸ“ models_v2/                  # ğŸ†• Architecture V2
â”‚   â””â”€â”€ ğŸ“„ latent_diffusion.py     # U-Net V2 + all encoders
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Data v1 (legacy)
â”‚   â”œâ”€â”€ ğŸ“„ music_dataset.py        # PyTorch Dataset
â”‚   â””â”€â”€ ğŸ“„ training_dataset.json   # Dataset v1
â”‚
â”œâ”€â”€ ğŸ“ data_v2/                    # ğŸ†• Data v2
â”‚   â”œâ”€â”€ ğŸ“„ segmented_dataset.py    # SegmentedMusicDataset
â”‚   â””â”€â”€ ğŸ“„ *.json                  # Datasets v2
â”‚
â”œâ”€â”€ ğŸ“ music/
â”‚   â””â”€â”€ ğŸ“ fma_small/              # FMA audio files
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/                # Checkpoints v1
â”œâ”€â”€ ğŸ“ checkpoints_v2/             # ğŸ†• Checkpoints v2
â”‚
â””â”€â”€ ğŸ“ output/                     # Generated audio
```

---

## Usage Scenarios

### Scenario 1: Training from Scratch on Your Own MP3s

**When to use:** You have your own MP3 collection and want to train a model from scratch.

#### Step 1: Prepare Folder Structure

```bash
mkdir -p my_music/artist_name
cp ~/Music/*.mp3 my_music/artist_name/
```

#### Step 2: Generate Dataset

```bash
# Full pipeline with audio analysis, vocals and voice embeddings
python build_dataset_v2.py \
    --audio_dir ./my_music \
    --output ./data_v2/my_dataset.json \
    --device cuda \
    --batch_size 4
```

**Generated files:**
- `my_dataset.json` - metadata + prompts + all audio features (CLAP, voice, F0, etc.)
- `my_dataset.artist_embeddings.json` - average voice embeddings per artist

#### Step 3: Train VAE (Phase 1)

```bash
python train_v2.py \
    --phase 1 \
    --annotations ./data_v2/my_dataset.json \
    --audio_dir ./my_music \
    --epochs 50 \
    --batch_size 4 \
    --device cuda
```

**Time:** ~2-4h for 1000 tracks (GPU RTX 3090)

#### Step 4: Train Diffusion (Phase 3)

```bash
python train_v2.py \
    --phase 3 \
    --annotations ./data_v2/my_dataset.json \
    --audio_dir ./my_music \
    --vae_checkpoint ./checkpoints/vae_epoch_50.pt \
    --epochs 100 \
    --batch_size 2 \
    --device cuda
```

**Time:** ~8-12h for 1000 tracks (GPU RTX 3090)

---

### Scenario 2: Training on FMA Dataset

**When to use:** You have the FMA dataset and want to train a model.

#### Step 1: Download FMA (if you don't have it)

```bash
# FMA Small (~8GB, 8000 tracks)
wget https://os.unil.cloud.switch.ch/fma/fma_small.zip
unzip fma_small.zip -d ./music/
```

#### Step 2: Build Dataset v2

```bash
python build_dataset_v2.py \
    --audio_dir ./music/fma_small \
    --output ./data_v2/fma_dataset.json \
    --device cuda \
    --batch_size 4
```

**What build_dataset_v2 generates:**
| Field | Description | Source |
|-------|-------------|--------|
| `has_vocals` | Whether track has vocals | Whisper |
| `lyrics` | Text transcription | Whisper |
| `voice_embedding` | 256-dim vector | Resemblyzer |
| `ecapa_embedding` | 192-dim vector | ECAPA-TDNN |
| `clap_audio_embedding` | 512-dim | CLAP |
| `clap_text_embedding` | 512-dim | CLAP |
| `f0_contour` | Pitch contour | CREPE/pYIN |
| `vibrato_*` | Vibrato features | Custom |
| `breath_positions` | Breath timings | Custom |
| `phoneme_timestamps` | IPA + timing | Gruut/eSpeak |

#### Step 3: Training

```bash
# Phase 1: VAE
python train_v2.py --phase 1 \
    --annotations ./data_v2/fma_dataset.json \
    --audio_dir ./music/fma_small \
    --epochs 50

# Phase 3: LDM with voice conditioning
python train_v2.py --phase 3 \
    --annotations ./data_v2/fma_dataset.json \
    --audio_dir ./music/fma_small \
    --vae_checkpoint ./checkpoints_v2/vae_best.pt \
    --epochs 100
```

---

### Scenario 3: Adding New Tracks to Dataset

**When to use:** You already have a dataset and want to add new tracks.

#### Method A: Rebuild with New Folder

```bash
# Add new MP3s to folder
cp ~/new_music/*.mp3 ./music/fma_small/new/

# Rebuild dataset (will detect new files)
python build_dataset_v2.py \
    --audio_dir ./music/fma_small \
    --output ./data_v2/dataset_updated.json \
    --device cuda
```

#### Method B: Merge JSON

```python
import json

# Load existing
with open('data_v2/dataset.json') as f:
    dataset = json.load(f)

# Load new
with open('data_v2/new_tracks.json') as f:
    new_tracks = json.load(f)

# Merge (check duplicates by audio_path)
existing_paths = {t['audio_path'] for t in dataset}
for track in new_tracks:
    if track['audio_path'] not in existing_paths:
        dataset.append(track)

# Save
with open('data_v2/dataset_merged.json', 'w') as f:
    json.dump(dataset, f, indent=2)
```

#### Method C: Continue Training (Fine-tuning)

```bash
# Fine-tune on new data
python train_v2.py --phase 3 \
    --annotations ./data_v2/dataset_merged.json \
    --audio_dir ./music \
    --vae_checkpoint ./checkpoints_v2/vae_best.pt \
    --ldm_checkpoint ./checkpoints_v2/ldm_epoch_100.pt \
    --epochs 20  # Fewer epochs for fine-tuning
```

---

## Inference - Music Generation

### Basic Generation

```bash
python inference_v2.py \
    --prompt "Energetic electronic dance track with heavy bass" \
    --output ./output/edm_track.wav \
    --duration 30 \
    --device cuda
```

### With Artist Style (Voice Embedding)

```bash
python inference_v2.py \
    --prompt "Melodic hip-hop beat" \
    --style_of "Artist Name" \
    --output ./output/artist_style.wav
```

### With Voice Cloning

```bash
python inference_v2.py \
    --prompt "Upbeat pop song" \
    --voice_clone "Artist Name" \
    --lyrics "Here are the lyrics to sing..." \
    --output ./output/cloned_voice.wav
```

### With Structure Template

```bash
python inference_v2.py \
    --prompt "Energetic pop with female vocals" \
    --template verse_chorus \
    --duration 120 \
    --output ./output/structured_song.wav
```

### All Options

```bash
python inference_v2.py --help

# Main options:
#   --prompt TEXT          Prompt describing the music
#   --output PATH          Output path (default: ./output/generated.wav)
#   --duration FLOAT       Duration in seconds (default: 30)
#   --cfg_scale FLOAT      Classifier-free guidance (default: 7.5)
#   --num_steps INT        Denoising steps (default: 50)
#   --template NAME        Structure template (verse_chorus, etc.)
#
# Voice conditioning:
#   --style_of NAME/PATH   Artist voice embedding or .wav file
#
# Voice cloning:
#   --voice_clone NAME     Artist to clone voice from
#   --voice_clone_samples PATH  Folder/file with voice samples
#   --lyrics TEXT          Text to sing
#   --language CODE        Language code (pl, en, de, etc.)
```

---

## Detailed File Descriptions

### ğŸ“„ `train_v2.py`

**Purpose:** Main v2 training script for VAE, Composition Planner and LDM.

**Training phases:**
1. **Phase 1 (VAE):** Audio â†’ Mel â†’ Latent â†’ Mel (reconstruction)
2. **Phase 2 (Composition Planner):** Track features â†’ Composition plan
3. **Phase 3 (LDM):** Noise â†’ UNet V2 (conditioned) â†’ Latent â†’ VAE â†’ Audio

**Key parameters:**
```python
# VAE
latent_dim = 128      # v2: increased from 8
sample_rate = 32000   # v2: 32kHz

# LDM
cfg_dropout = 0.1     # Classifier-free guidance dropout
voice_dropout = 0.1   # Voice conditioning dropout
```

---

### ğŸ“„ `inference_v2.py`

**Purpose:** Generate music from trained v2 model.

**Main functions:**
- `generate_composition_plan()` - plan track structure
- `generate_section_audio()` - generate single section
- `generate_full_song()` - generate full track section by section

**Pipeline:**
1. Prompt â†’ T5/CLAP Encoder â†’ text embedding
2. (optional) Voice sample â†’ Resemblyzer/ECAPA â†’ voice embedding
3. (optional) Lyrics â†’ Gruut/eSpeak â†’ phonemes IPA
4. Template â†’ CompositionPlanner â†’ section structure
5. Per section: Noise + embeddings â†’ UNet V2 denoising â†’ Latent
6. Latent â†’ VAE Decoder â†’ Mel spectrogram
7. Mel â†’ HiFi-GAN â†’ Audio WAV
8. Concat all sections â†’ Final audio

---

### ğŸ“„ `build_dataset_v2.py`

**Purpose:** Full feature extraction from audio files.

**Extracts:**
- Metadata (ID3 tags)
- Audio features (librosa: tempo, key, energy, etc.)
- Voice embeddings (Resemblyzer 256-dim + ECAPA-TDNN 192-dim)
- CLAP embeddings (audio 512-dim + text 512-dim)
- Pitch/F0 (CREPE/pYIN)
- Vibrato, breath, phoneme features
- Segment detection (verse/chorus/bridge)
- Lyrics transcription (Whisper)

**Output:** JSON with v3.1 fields (see DATASET diagram above)

---

### ğŸ“„ `models_v2/latent_diffusion.py`

**Purpose:** UNet V2 + all conditioning modules.

**Main classes:**
- `UNetV2` - main diffusion model
- `SectionConditioningModule` - fusion of all conditioning
- `VoiceStreamAttention` - gated cross-attention for voice
- `VoiceEmbeddingFusion` - Resemblyzer + ECAPA fusion
- `PitchEncoder`, `VibratoEncoder`, `BreathEncoder` - feature encoders
- `BeatEmbedding`, `ChordEmbedding`, `PhonemeEncoder` - v2 encoders

---

### ğŸ“„ `models/audio_vae.py`

**Purpose:** Audio compression to latent space.

**Architecture v2:**
```
Mel [1, 128, T] â†’ Encoder â†’ Î¼, Ïƒ â†’ z [128, H, W] â†’ Decoder â†’ Mel [1, 128, T]
```

**Parameters:**
- `latent_dim = 8` - latent channel dimension
- `channels = [64, 128, 256, 512]` - encoder channels
- `n_mels = 128` - number of mel filterbanks (v2: increased from 80)

**Loss:**
```python
loss = reconstruction_loss + beta * kl_divergence + stft_loss
```

---

### ğŸ“„ `models/text_encoder.py`

**Purpose:** Text prompt encoding.

**Backends:**
- `T5TextEncoder` - Flan-T5 (768-dim, good for long descriptions)
- `CLAPTextEncoder` - CLAP (specifically trained on audio-text)

---

### ğŸ“„ `models/voice_synthesis.py`

**Purpose:** Voice cloning and singing voice synthesis.

**Supported Backends:**
| Backend | Type | License | Quality | Notes |
|---------|------|---------|---------|-------|
| **fish_speech** | Zero-shot TTS | Apache 2.0 | â­â­â­â­â­ | #1 TTS-Arena2, emotions, 10-30s sample |
| **gpt_sovits** | Zero/Few-shot TTS | MIT | â­â­â­â­â­ | SOTA, 5s sample, EN/JA/KO/ZH |
| coqui | XTTS v2 | Apache 2.0 | â­â­â­â­ | Local, multilingual |
| elevenlabs | API | Commercial | â­â­â­â­â­ | Best quality, paid |
| bark | Local | MIT | â­â­â­ | Experimental |
| rvc | Voice conversion | MIT | â­â­â­â­ | Requires source audio |

**Fish Speech Setup (Recommended - #1 Quality):**
```bash
# Option 1: Use Fish Audio cloud (easiest)
# Get API key from https://fish.audio
VoiceSynthesizer(backend="fish_speech", api_key="YOUR_KEY")

# Option 2: Local server
pip install fish-speech
python -m fish_speech.webui.api --listen 0.0.0.0:8080

# Option 3: HuggingFace Spaces (free demo)
# https://huggingface.co/spaces/fishaudio/fish-speech-1
```

**Fish Speech Emotion Markers:**
```python
# Basic emotions
"(angry) I'm so frustrated! (sighing)"
"(excited) We won the championship! (laughing)"
"(sad) I miss you so much (sobbing)"

# Tones
"(whispering) This is a secret"
"(shouting) Stop right there!"

# Effects  
"(laughing)", "(crying loudly)", "(sighing)", "(panting)"
```

**GPT-SoVITS Setup:**
```bash
# Clone repo
git clone https://github.com/RVC-Boss/GPT-SoVITS.git
cd GPT-SoVITS

# Install (choose CUDA version)
bash install.sh --device CU126

# Start API server
python api_v2.py -a 0.0.0.0 -p 9880
```

**Usage with GPT-SoVITS:**
```python
from models.voice_synthesis import VoiceSynthesizer

# Initialize with GPT-SoVITS
synth = VoiceSynthesizer(
    backend="gpt_sovits",
    gpt_sovits_url="http://localhost:9880"
)

# Register voice (only needs 5 seconds!)
synth.register_voice("singer", "voice_5sec.wav")

# Synthesize singing
audio = synth.synthesize(
    text="I walk alone through empty streets",
    voice="singer",
    language="en"
)
```

**Fish Speech Usage (alternative, #1 TTS-Arena2):**
```python
from models.voice_synthesis import VoiceSynthesizer

# Initialize with Fish Speech
synth = VoiceSynthesizer(
    backend="fish_speech",
    fish_speech_url="http://localhost:8080"  # Local server
    # Or use cloud: api_key="your_fish_audio_key"
)

# Register voice (10-30s sample)
synth.register_voice("singer", "voice_30sec.wav")

# Synthesize with emotion markers!
audio = synth.synthesize(
    text="(excited) I can't believe we won! (laughing)",
    voice="singer",
    language="en"
)
```

**CLI Usage (inference_v2.py):**
```bash
# Generate instrumental + synthesized vocals (GPT-SoVITS)
python inference_v2.py \
    --prompt "Epic ballad with dramatic strings" \
    --lyrics "I walk alone through empty streets" \
    --sing_lyrics \
    --singing_voice_ref ./voice_sample.wav \
    --singing_backend gpt_sovits \
    --gpt_sovits_url http://localhost:9880 \
    --mix_vocals 0.7 \
    --duration 120

# Using Fish Speech (best quality, #1 TTS-Arena2)
python inference_v2.py \
    --prompt "Emotional pop song" \
    --lyrics "(sad) Every moment I think of you (sighing)" \
    --sing_lyrics \
    --singing_voice_ref ./voice_sample.wav \
    --singing_backend fish_speech \
    --fish_speech_url http://localhost:8080 \
    --duration 120

# Using Fish Audio cloud API
python inference_v2.py \
    --prompt "Dance track" \
    --lyrics "(excited) Let's go!" \
    --sing_lyrics \
    --singing_voice_ref ./voice_sample.wav \
    --singing_backend fish_speech \
    --fish_speech_api_key YOUR_API_KEY \
    --duration 120
```

---

### ğŸ¯ Vocal Alignment Pipeline

**Problem:** TTS generuje wokal bez wiedzy o strukturze piosenki - gdzie intro, verse, chorus?

**RozwiÄ…zanie:** Inteligentny pipeline alignmentu wokali do instrumentalu.

#### Flow syntezy wokali:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: LDM generuje instrumental (z phonemes conditioning)            â”‚
â”‚          â†“                                                              â”‚
â”‚  STEP 2: Demucs stripuje przypadkowe wokale z LDM (--strip_ldm_vocals)  â”‚
â”‚          â†“                                                              â”‚
â”‚  STEP 3: Wykrywanie regionÃ³w wokalnych                                  â”‚
â”‚          - Priorytet: CompositionPlan (wie gdzie verse/chorus!)         â”‚
â”‚          - Fallback: Beat detection + energy analysis                   â”‚
â”‚          â†“                                                              â”‚
â”‚  STEP 4: GPT-SoVITS / Fish Speech generuje TTS wokal                    â”‚
â”‚          â†“                                                              â”‚
â”‚  STEP 5: Alignment - dopasowanie wokali do regionÃ³w                     â”‚
â”‚          - Time stretch (0.77x - 1.3x, naturalny zakres)                â”‚
â”‚          - Proporcjonalna dystrybucja po sekcjach                       â”‚
â”‚          - 30ms fades dla gÅ‚adkich przejÅ›Ä‡                              â”‚
â”‚          â†“                                                              â”‚
â”‚  STEP 6: Mix instrumental + aligned vocals                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Wykrywanie regionÃ³w (`detect_vocal_regions()`):

**1. Z CompositionPlan (najlepsze):**
```python
# Plan wie gdzie sÄ… sekcje z wokalami:
plan.sections = [
    Section(type="intro", duration=8, has_vocals=False),   # â† pomijamy
    Section(type="verse", duration=32, has_vocals=True),   # â† WOKAL
    Section(type="chorus", duration=24, has_vocals=True),  # â† WOKAL
    Section(type="bridge", duration=16, has_vocals=False), # â† pomijamy
]

# Generowane regiony:
[(8.5, 39.5), (40.5, 63.5)]  # 0.5s offset na poczÄ…tku/koÅ„cu
```

**2. Fallback - Beat + Energy analysis:**
```python
# JeÅ›li plan nie dostÄ™pny:
# 1. Librosa beat tracking â†’ snap do downbeatÃ³w
# 2. Energy analysis â†’ regiony o umiarkowanej energii (10-80%)
```

#### Alignment wokali (`align_vocals_to_instrumental()`):

| Parametr | WartoÅ›Ä‡ | Opis |
|----------|---------|------|
| Time stretch min | 0.77x | Maksymalne przyspieszenie (naturalne) |
| Time stretch max | 1.3x | Maksymalne spowolnienie |
| Fade duration | 30ms | GÅ‚adkie przejÅ›cia miÄ™dzy sekcjami |
| Section offset | 0.5s | OpÃ³Åºnienie na poczÄ…tku kaÅ¼dej sekcji |

#### CLI flagi alignmentu:

```bash
# DomyÅ›lne (bezpieczne - stripuje LDM vocals, uÅ¼ywa planu)
python inference_v2.py \
    --prompt "Rock ballad" \
    --lyrics "I walk alone..." \
    --sing_lyrics \
    --singing_voice_ref voice.wav

# Szybkie (bez Demucs - dla czystych instrumentali)
python inference_v2.py \
    --prompt "Instrumental jazz" \
    --lyrics "..." \
    --sing_lyrics \
    --no_strip_ldm_vocals \
    --singing_voice_ref voice.wav
```

#### Pliki wyjÅ›ciowe:

```
output/
  â”œâ”€â”€ output.wav          # Finalny mix (instrumental + aligned vocals)
  â”œâ”€â”€ output_vocals.wav   # Tylko aligned vocals (do debugowania)
  â””â”€â”€ output.json         # Metadata z planem sekcji
```

---

### ğŸ“„ `data/music_dataset.py`

**Purpose:** PyTorch Dataset for training.

**Returns batch:**
```python
{
    'audio': torch.Tensor,           # [num_samples]
    'prompt': str,                   # "Energetic rock song..."
    'voice_embedding': torch.Tensor, # [256] or None
    'lyrics': str,                   # "Transcribed lyrics..."
    'has_vocals': bool,
    'text_sentiment': str,           # "positive"
    'track_id': int,
    'artist': str,
}
```

**Custom collate_fn:**
- Stacks tensors
- Groups strings into lists
- Handles None in voice_embedding

---

## FAQ & Troubleshooting

### â“ Why `/var/folders/...` in vocals path?

**Question:** `Vocals saved to: /var/folders/fg/frwh54994k9gy6h5y_tc1_940000gn/T/2_Food_vocals.wav`

**Answer:** This is the **default macOS temporary folder** (`tempfile.gettempdir()`).

`VoiceExtractorFromSong` saves extracted vocals to the system temporary folder by default, which on macOS is:
```
/var/folders/XX/XXXX/T/
```

**Solution:** Set your own `output_dir`:

```python
extractor = VoiceExtractorFromSong(
    output_dir="./data/separated_vocals"  # Permanent folder
)
```

Or when building dataset with `build_dataset_v2.py` use `--separate_vocals` flag.

---

### â“ Training is Very Slow on CPU

**Problem:** Training on CPU takes hours even for a few tracks.

**Solutions:**
1. Use GPU: `--device cuda`
2. Reduce batch size: `--batch_size 1`
3. Reduce number of tracks: `--max_tracks 10`
4. Use mixed precision (auto on GPU)

---

### â“ `CUDA out of memory`

**Problem:** GPU doesn't have enough memory.

**Solutions:**
1. Reduce batch size: `--batch_size 1`
2. Use gradient checkpointing (enabled by default)
3. Use smaller VAE model
4. Shorten duration: change in code `duration=5.0`

---

### â“ Voice Cloning Sounds Robotic

**Problem:** XTTS generates artificial voice.

**Solutions:**
1. Use longer voice sample (>30s)
2. Make sure sample has clean vocals (no instruments)
3. Use ElevenLabs instead of Coqui (better quality, paid)

---

### â“ Whisper Doesn't Detect Vocals

**Problem:** `has_vocals: false` for tracks with vocals.

**Causes:**
1. Instrumental too loud
2. Vocals in unsupported language
3. Analyzed fragment too short

**Solutions:**
1. Use `--whisper_full` (analyze entire track)
2. Use larger model: `--whisper_model medium`
3. First separate vocals: `--separate_vocals`

---

### â“ Missing Module `speechbrain`

**Warning:** `No module named 'speechbrain'`

**Solution:** System automatically uses `resemblyzer` as fallback. If you want SpeechBrain:
```bash
pip install speechbrain
```

---

## ğŸ“Š Model Size Configuration

### Model Size Parameters

| Parameter | Impact | Description |
|-----------|--------|-------------|
| `latent_dim` | Minimal (~3M) | VAE latent space dimension |
| `model_channels` | **KEY** | Base UNet channel width - main "size knob" |

### Model Size Table

| Config | latent_dim | model_channels | VAE | UNet | **Total** |
|--------|-----------|----------------|-----|------|-----------|
| Test/Dev | 128 | 256 | 224M | 722M | **~1B** |
| Production Default | 128 | 320 | 224M | 1.1B | **~1.3B** |
| Large Production | 128 | 512 | 224M | 2.8B | **~3B** |
| XL Production | 256 | 512 | 228M | 2.8B | **~3B** |
| XXL (multi-billion) | 256 | 768 | 228M | 6.1B | **~6.4B** |

### Conclusions

- **`latent_dim=128` is sufficient** - difference between 128 and 256 is only ~3M parameters in VAE (~1.5% difference)
- **`model_channels` is the real "size knob"** - increasing from 320â†’512 gives jump from 1.1Bâ†’2.8B
- For **several billion parameters**: `model_channels=512-768` is key

### Recommendations

| Use Case | Configuration | Size |
|----------|---------------|------|
| Local testing/dev | `latent_dim=128, model_channels=256` | ~1B |
| Standard production | `latent_dim=128, model_channels=320` | ~1.3B |
| Large production model | `latent_dim=128, model_channels=512` | ~3B |
| Very large model | `latent_dim=256, model_channels=768` | ~6.4B |

### Code Configuration Example

```python
# Test/Dev (~1B)
unet = UNetV2(
    in_channels=128,
    out_channels=128,
    model_channels=256,  # smaller for quick testing
    context_dim=768,
)

# Production (~3B)
unet = UNetV2(
    in_channels=128,
    out_channels=128,
    model_channels=512,  # larger for quality
    context_dim=768,
)
```

### AudioVAE - Full Configuration

**`AudioVAE.__init__` parameters:**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `sample_rate` | 32000 | v2: 32kHz (v1: 22050) |
| `n_mels` | 128 | Number of mel bins |
| `n_fft` | 1024 | FFT window size |
| `hop_length` | 320 | 10ms hop @ 32kHz |
| `latent_dim` | 128 | v2: increased from 8 |
| `channels` | None | Auto-select from `LATENT_CONFIGS` |
| `use_stft_loss` | True | Multi-Resolution STFT Loss |
| `use_checkpoint` | False | Gradient checkpointing (saves VRAM) |

**Auto-select channels (`LATENT_CONFIGS`):**

| latent_dim | channels (auto) | VAE Size |
|------------|-----------------|----------|
| 8 | [64, 128, 256, 512] | **55M** |
| 32 | [64, 128, 256, 512] | **56M** |
| 64 | [96, 192, 384, 768] | **125M** |
| 128 | [128, 256, 512, 1024] | **224M** |

**Custom channels - full scale:**

| Config | channels | Size |
|--------|----------|------|
| v2 Light | [64, 128, 256, 512] | **57M** |
| v2 Default | [128, 256, 512, 1024] | **224M** |
| v2 Heavy | [256, 512, 1024, 2048] | **889M** |

**VAE configuration examples:**

```python
# Default v2 (224M) - recommended
vae = AudioVAE(latent_dim=128)

# Light (57M) - quick tests
vae = AudioVAE(latent_dim=128, channels=[64, 128, 256, 512])

# Heavy (889M) - maximum reconstruction quality
vae = AudioVAE(latent_dim=128, channels=[256, 512, 1024, 2048])

# With gradient checkpointing (less VRAM)
vae = AudioVAE(latent_dim=128, use_checkpoint=True)
```

---

## Requirements

```txt
# Core
torch>=2.0
torchaudio>=2.0
transformers>=4.30
einops
vocos

# Audio processing
librosa
soundfile
mutagen

# Whisper (optional)
faster-whisper  # or openai-whisper

# Voice embeddings (one of):
resemblyzer        # lightweight (256-dim)
speechbrain        # better (192-dim ECAPA-TDNN)

# Voice cloning (optional)
TTS                # Coqui XTTS v2
demucs             # Vocal separation

# LLM (optional)
openai             # GPT-4
requests           # Ollama
```

---

## License

GPL-3.0 License - use for your own projects!

âš ï¸ **Legal notice:** Voice cloning may violate artists' voice likeness rights. Use only with your own voice or with the owner's consent.

---

## Related Documents

- ğŸ“˜ [Dataset Builder - Full Documentation](docs_v2/DATASET_BUILDER.md)

---

*Documentation generated: December 14, 2025*

