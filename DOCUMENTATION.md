# Muzible Muze AI - Technical Documentation v2

> **Text-to-Music Generation with Latent Diffusion & Voice Conditioning**

---

## Table of Contents

1. [System Architecture](#system-architecture)
2. [V2 Architecture - Voice Stream Attention](#v2-architecture---voice-stream-attention)
3. [Training Pipeline](#training-pipeline)
4. [Dataset Format](#dataset-format)
5. [Conditioning System](#conditioning-system)
6. [File Structure](#file-structure)
7. [Usage Scenarios](#usage-scenarios)
8. [Inference - Music Generation](#inference---music-generation)
9. [Detailed File Descriptions](#detailed-file-descriptions)
10. [FAQ & Troubleshooting](#faq--troubleshooting)
11. [Model Size Configuration](#model-size-configuration)
12. [Requirements](#requirements)

---

## System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         MUZIBLE MUZE AI v2                                  ‚îÇ
‚îÇ                   Text-to-Music Generation Pipeline                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ   INPUTS     ‚îÇ    ‚îÇ   ENCODERS   ‚îÇ    ‚îÇ   OUTPUTS    ‚îÇ                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  ‚îÇ
‚îÇ  ‚îÇ Text Prompt  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ T5/CLAP      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ              ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ Voice Sample ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Resemblyzer  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  UNet V2     ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ Style Ref    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ECAPA-TDNN   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Diffusion) ‚îÇ‚îÄ‚îÄ‚ñ∂ Audio WAV    ‚îÇ
‚îÇ  ‚îÇ Lyrics       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Gruut/eSpeak ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ              ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                    CORE COMPONENTS                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
‚îÇ  ‚îÇ  AudioVAE (224M)  ‚îÇ  UNet V2 (722M-6.1B)  ‚îÇ  Vocos Vocoder          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Mel ‚Üí Latent   ‚îÇ  - Noise ‚Üí Latent     ‚îÇ  - Mel ‚Üí Waveform       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Latent ‚Üí Mel   ‚îÇ  - Voice Attention    ‚îÇ  - 44.1kHz output       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - KL + STFT Loss ‚îÇ  - Section Cond.      ‚îÇ  - High quality         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Main Models

| Component | Parameters | Function |
|-----------|------------|----------|
| **AudioVAE** | 55-889M | Audio compression to latent space |
| **UNet V2** | 722M-6.1B | Latent diffusion denoising |
| **T5 Encoder** | 250M | Text prompt encoding |
| **CLAP** | 600M | Audio-text joint embeddings |
| **Vocos** | 13M | High-quality vocoder |

---

## V2 Architecture - Voice Stream Attention

### What is VoiceStreamAttention?

**VoiceStreamAttention** is a **dedicated cross-attention mechanism** that allows the diffusion model to attend to voice embedding **separately** from text embedding.

```
Standard Cross-Attention (v1):
    Q = latent, K,V = text_embedding
    
V2 Voice Stream Attention:
    Branch 1: Q = latent, K,V = text_embedding      ‚Üí text_attn
    Branch 2: Q = latent, K,V = voice_embedding     ‚Üí voice_attn
    Output: gate * voice_attn + (1-gate) * text_attn
```

### Why is it important?

1. **Voice quality** - Model can "focus" on voice characteristics independently
2. **Timbre control** - Voice gate allows dynamic balance between text and voice
3. **Better disentanglement** - Voice separated from semantics

### V2 Architecture Diagram

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ              UNet V2 Block                   ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
Input Latent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ResBlock  ‚îÇ  Self-Attn  ‚îÇ  Cross-Attn     ‚îÇ
    [B,128,H,W]     ‚îÇ            ‚îÇ             ‚îÇ                  ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ Text K,V   ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ [B,768]    ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ         ‚îÇ        ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ text_attn  ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ         ‚îÇ        ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ GATED MIX  ‚îÇ‚óÄ‚îÄ‚îÄ gate (learnable)
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ         ‚îÇ        ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ voice_attn ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ         ‚îÇ        ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ Voice K,V  ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îÇ [B,256]    ‚îÇ ‚îÇ
                    ‚îÇ            ‚îÇ             ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
                              Output Latent [B,128,H,W]
```

### VoiceEmbeddingFusion (v2)

In v2, we use **two voice embeddings**:

| Embedding | Dimension | Model | Characteristics |
|-----------|-----------|-------|-----------------|
| **Resemblyzer** | 256 | GE2E | General speaker verification |
| **ECAPA-TDNN** | 192 | SpeechBrain | Better for singing voice |

```python
# Fusion
voice_fused = VoiceEmbeddingFusion(
    resemblyzer_embed,  # [B, 256]
    ecapa_embed         # [B, 192]
)
# Output: [B, 256] - weighted projection
```

---

## Training Pipeline

### Phase Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        TRAINING PIPELINE v2                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                            ‚îÇ
‚îÇ  Phase 1: VAE (Audio Compression)                                          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  Audio WAV ‚Üí Mel Spectrogram ‚Üí Encoder ‚Üí Œº, œÉ ‚Üí z (latent) ‚Üí Decoder ‚Üí Mel ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  Loss: MSE(mel, mel_recon) + Œ≤*KL(z) + STFT_loss                          ‚îÇ
‚îÇ  Target: Reconstruct audio with minimal latent dim (128)                   ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                            ‚îÇ
‚îÇ  Phase 2: Composition Planner (Optional)                                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ
‚îÇ  Track features ‚Üí MLP ‚Üí Section plan (verse, chorus, bridge, etc.)         ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  Loss: CrossEntropy(predicted_sections, ground_truth_sections)             ‚îÇ
‚îÇ  Target: Learn song structure from metadata                                ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                            ‚îÇ
‚îÇ  Phase 3: Latent Diffusion Model (LDM)                                     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ  Noise z_T ‚Üí UNet V2 (conditioned) ‚Üí ... ‚Üí z_0 ‚Üí VAE Decode ‚Üí Audio        ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  Conditioning:                                                             ‚îÇ
‚îÇ  - Text: T5/CLAP embedding [768]                                           ‚îÇ
‚îÇ  - Voice: Resemblyzer [256] + ECAPA [192]                                  ‚îÇ
‚îÇ  - Section: type, position, energy, tempo, key                             ‚îÇ
‚îÇ  - Audio: CLAP audio embedding [512]                                       ‚îÇ
‚îÇ  - Beat/Chord/Phoneme encoders                                             ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  Loss: MSE(predicted_noise, actual_noise) + cfg_loss                       ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### LDM Training with All Conditioning

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LDM v2 TRAINING - FULL CONDITIONING                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                            ‚îÇ
‚îÇ  INPUTS (per batch):                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ audio_path: "music/fma_small/000/000123.mp3"                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ prompt: "Energetic rock with electric guitar and drums"              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ section_type: "chorus"                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ position: 0.35                                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ energy: 0.82                                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ tempo: 128.0                                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ key: "C major"                                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ voice_embedding: [256-dim tensor]                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ecapa_embedding: [192-dim tensor]                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ clap_audio_embedding: [512-dim tensor]                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ clap_text_embedding: [512-dim tensor]                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ num_beats: 64                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ beat_positions: [[0.0, 0.47], [0.47, 0.94], ...]                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ current_chord: "C:maj"                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ phonemes_ipa: "√∞…™s …™z …ô t…õst"                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ f0_contour: [440.0, 442.1, ...]                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ vibrato_rate, vibrato_depth, breath_positions, ...                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  PROCESSING:                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  1. Load audio ‚Üí Mel spectrogram                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  2. VAE.encode(mel) ‚Üí z_0 (latent)                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  3. Sample timestep t ~ Uniform(0, T)                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  4. Add noise: z_t = ‚àö·æ±‚Çú¬∑z_0 + ‚àö(1-·æ±‚Çú)¬∑Œµ                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  5. Encode conditioning:                                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     - text_embed = T5(prompt)           [768]                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     - voice_fused = Fusion(voice, ecapa) [256]                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     - section_cond = SectionModule(...)  [1024]                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  6. UNet forward: Œµ_Œ∏ = UNet(z_t, t, text_embed, voice_fused, ...)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  7. Loss = MSE(Œµ_Œ∏, Œµ)                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Dataset Format

### Dataset JSON Structure (v3.1)

```json
{
  "audio_path": "music/fma_small/000/000123.mp3",
  "track_id": 123,
  "artist": "Artist Name",
  "title": "Song Title",
  "album": "Album Name",
  "genre": "rock",
  "year": 2023,
  
  "prompt": "Energetic rock song with electric guitar riffs and powerful drums...",
  "text_sentiment": "positive",
  
  "duration": 180.5,
  "sample_rate": 32000,
  "tempo": 128.0,
  "key": "C major",
  "time_signature": "4/4",
  "loudness_db": -8.5,
  "energy": 0.82,
  
  "has_vocals": true,
  "lyrics": "Transcribed lyrics from the song...",
  "phonemes_ipa": "√∞…™s …™z √∞…ô fa…™n…ôl transkr…™p É…ôn",
  
  "voice_embedding": [0.12, -0.34, ...],
  "voice_embedding_separated": [0.15, -0.31, ...],
  
  "clap_audio_embedding": [0.05, 0.12, ...],
  "clap_text_embedding": [0.08, 0.15, ...],
  
  "segments": [
    {
      "type": "intro",
      "start": 0.0,
      "end": 15.2,
      "energy": 0.3,
      "has_vocals": false
    },
    {
      "type": "verse",
      "start": 15.2,
      "end": 45.8,
      "energy": 0.6,
      "has_vocals": true,
      "lyrics": "First verse lyrics..."
    },
    {
      "type": "chorus",
      "start": 45.8,
      "end": 76.4,
      "energy": 0.9,
      "has_vocals": true,
      "lyrics": "Chorus lyrics..."
    }
  ],
  
  "beat_positions": [[0.0, 0.47], [0.47, 0.94], ...],
  "downbeat_positions": [0.0, 1.88, 3.76, ...],
  "chord_progression": ["C:maj", "G:maj", "Am:min", "F:maj"],
  
  "f0_contour": [440.0, 442.1, 438.5, ...],
  "f0_voiced_mask": [true, true, false, ...],
  "vibrato_rate": 5.2,
  "vibrato_depth": 0.15,
  "vibrato_extent": 0.8,
  "breath_positions": [[12.5, 12.8], [25.1, 25.4], ...],
  "phoneme_timestamps": [
    {"phoneme": "√∞", "start": 0.0, "end": 0.05},
    {"phoneme": "…™", "start": 0.05, "end": 0.12}
  ]
}
```

---

## Conditioning System

### Conditioning Summary

| Parameter | Type | Dimension | Encoder |
|-----------|------|-----------|---------|
| `prompt` | str | ‚Üí 768 | T5TextEncoder |
| `section_type` | str | ‚Üí 128 | SectionEmbedding |
| `position` | float 0-1 | ‚Üí 128 | SinusoidalPosEmb |
| `energy` | float 0-1 | ‚Üí 64 | Linear |
| `tempo` | float BPM | ‚Üí 64 | Linear (normalized) |
| `key` | int 0-23 | ‚Üí 64 | KeyEmbedding |
| `loudness` | float dB | ‚Üí 64 | Linear |
| `has_vocals` | bool | ‚Üí 32 | Linear |
| `sentiment` | str | ‚Üí 64 | SentimentEmbedding |
| `genre` | str | ‚Üí 64 | GenreEmbedding |
| `artist` | str | ‚Üí 64 | ArtistEmbedding |
| `clap_audio` | 512-dim | ‚Üí 128 | Linear projection |
| `clap_text` | 512-dim | ‚Üí 128 | Linear projection |
| `voice_embedding` | 256-dim | ‚Üí 256 | VoiceStreamAttention |
| `ecapa_embedding` | 192-dim | ‚Üí 256 | VoiceEmbeddingFusion |
| `num_beats` | int | ‚Üí 64 | BeatEmbedding |
| `beat_positions` | List[List[float]] | ‚Üí 64 | BeatEmbedding |
| `time_signature` | str | ‚Üí 32 | TimeSignatureEmb |
| `current_chord` | str | ‚Üí 64 | ChordEmbedding |
| `phonemes_ipa` | str | ‚Üí 128 | PhonemeEncoder (GRU) |
| `f0_contour` | List[float] | ‚Üí 64 | F0Encoder (Conv1d) |
| `f0_voiced_mask` | List[bool] | ‚Üí 32 | VoicedMaskEncoder |
| `vibrato_rate` | float Hz | ‚Üí 64 | VibratoEncoder |
| `vibrato_depth` | float cents | ‚Üí 64 | VibratoEncoder |
| `vibrato_extent` | float 0-1 | ‚Üí 64 | VibratoEncoder |
| `breath_positions` | List[List[float]] | ‚Üí 32 | BreathEncoder |

### Fusion Dimensions

```
Base:     section(128) + position(128) + energy(64) + tempo(64) + key(64) + text(512)
          + loudness(64) + has_vocals(32) + sentiment(64) + genre(64) + artist(64) = 1248

Optional: + clap(128) + beat(64) + chord(64) + phoneme(128)
          + pitch(64) + vibrato(64) + breath(32) + phoneme_ts(64) = 1856

Final:    Fusion MLP ‚Üí output_dim (1024)
```

---

## Inference Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      INFERENCE PIPELINE v2                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                            ‚îÇ
‚îÇ  INPUT: "Energetic rock song with female vocals"                           ‚îÇ
‚îÇ         + voice_sample.wav (optional)                                      ‚îÇ
‚îÇ         + lyrics (optional)                                                ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 1: Text Encoding                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   prompt ‚Üí T5Encoder ‚Üí text_embed [768]                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   prompt ‚Üí CLAPTextEncoder ‚Üí clap_text_embed [512]                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                                ‚îÇ
‚îÇ                           ‚ñº                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 2: Voice Encoding (if voice_sample provided)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   voice.wav ‚Üí Resemblyzer ‚Üí voice_embed [256]                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   voice.wav ‚Üí ECAPA-TDNN ‚Üí ecapa_embed [192]                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Fusion(voice, ecapa) ‚Üí voice_fused [256]                          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                                ‚îÇ
‚îÇ                           ‚ñº                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 3: Composition Planning                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Template "verse_chorus" ‚Üí [intro, verse, chorus, verse, chorus]   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Each section: duration, energy, position                          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                                ‚îÇ
‚îÇ                           ‚ñº                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 4: Per-Section Generation (DDPM/DDIM)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   For each section:                                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     z_T ~ N(0, I)                     # Start with noise            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     for t = T, T-1, ..., 1:                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       Œµ_Œ∏ = UNet(z_t, t, text_embed, voice_fused, section_cond)    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       z_{t-1} = DDPM_step(z_t, Œµ_Œ∏, t)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     z_0 = final denoised latent                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                                ‚îÇ
‚îÇ                           ‚ñº                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 5: Audio Decoding                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   z_0 ‚Üí VAE.decode() ‚Üí mel_spectrogram [128, T]                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   mel ‚Üí Vocos ‚Üí audio_waveform [samples]                            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                                ‚îÇ
‚îÇ                           ‚ñº                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Step 6: Concatenation                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   [intro_audio, verse_audio, chorus_audio, ...] ‚Üí final_audio.wav   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Apply crossfade between sections (50ms)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îÇ  OUTPUT: final_audio.wav (44.1kHz stereo)                                  ‚îÇ
‚îÇ                                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### DDPM vs DDIM

| Method | Steps | Speed | Quality |
|--------|-------|-------|---------|
| DDPM | 1000 | Slow (~2min/30s) | Best |
| DDPM-50 | 50 | Medium (~15s/30s) | Good |
| DDIM-50 | 50 | Medium (~15s/30s) | Good |
| DDIM-20 | 20 | Fast (~6s/30s) | Acceptable |

**Recommendation:** Use DDIM-50 for production, DDPM-1000 for final renders.

---

## UNet V2 - Key Modules

```python
UNetV2(
    in_channels=128,        # latent_dim from VAE (v2: increased from 8)
    out_channels=128,
    model_channels=320,     # main "size knob"
    num_res_blocks=2,
    attention_resolutions=[8, 4, 2],
    context_dim=768,        # text embedding dim
    num_heads=8,
    
    # v2: Voice conditioning
    voice_dim=256,              # Resemblyzer embedding
    ecapa_dim=192,              # ECAPA-TDNN embedding (voice_emb_separated)
    clap_dim=512,               # CLAP audio+text embedding
    use_voice_stream=True,      # VoiceStreamAttention
    use_dual_voice=True,        # Resemblyzer + ECAPA fusion
    
    # v2: Beat/Chord/Phoneme
    use_clap=True,
    use_beat=True,              # BeatEmbedding
    use_chord=True,             # ChordEmbedding
    use_phonemes=True,          # PhonemeEncoder
    
    # v3: Pitch conditioning
    use_pitch=True,
    f0_encoder_dim=64,
    
    # v3.1: Singing expression
    vibrato_encoder_dim=64,
    breath_encoder_dim=32,
    phoneme_timestamp_encoder_dim=64,
    
    # Performance
    use_gradient_checkpointing=True,
)
```

### SectionConditioningModule - Sections and Metadata

```python
SectionConditioningModule(
    output_dim=1024,
    text_embed_dim=768,
    section_embed_dim=128,
    num_keys=24,            # C-B major/minor
    
    # v2 modules:
    use_clap=True,
    use_beat=True,
    use_chord=True,
    use_phonemes=True,
    
    # v3 modules:
    use_pitch=True,
    clap_dim=512,
    voice_dim=256,
)

# Forward accepts 30+ conditioning parameters
section_cond.forward(
    text_embed,                     # [B, 768] or [B, seq, 768]
    section_type,                   # List[str]
    position, energy, tempo,        # [B] floats
    key_idx,                        # [B] int 0-23
    loudness, has_vocals,           # [B] v3 metadata
    sentiment_score, genres, artists,
    clap_audio_embedding, clap_text_embedding,
    num_beats, beat_positions, time_signature, current_chord,
    phonemes_ipa, voice_embedding,
    f0, f0_coarse, f0_voiced_mask,
    vibrato_rate, vibrato_depth, vibrato_extent,
    breath_positions, phoneme_timestamps,
    segment_duration,
)
‚Üí (conditioning [B, 1024], phoneme_durations or None)
```

---

## File Structure

```
muzible-muze-ai/
‚îú‚îÄ‚îÄ üìÑ train_v2.py                 # Training script v2 (3-phase)
‚îú‚îÄ‚îÄ üìÑ inference_v2.py             # Music generation from model
‚îú‚îÄ‚îÄ üìÑ build_dataset_v2.py         # Dataset builder v2 (full extraction)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs_v2/                    # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ DATASET_BUILDER.md      # Full dataset builder documentation
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tools/
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ f0_extractor.py         # F0/pitch extraction
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ analyze_metadata.py     # Metadata analysis
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tools_v2/                   # Tools v2
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ segment_annotator.py    # Segment detection (verse/chorus)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ generate_artist_embeddings.py  # Voice embeddings generation
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scan_mp3_folder.py      # MP3 folder scanning
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ audio_vae.py            # Audio VAE (audio ‚Üí latent compression)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vocoder.py              # Vocoder (mel ‚Üí waveform)
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ voice_synthesis.py      # Voice cloning (XTTS, Demucs)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models_v2/                  # üÜï Architecture V2
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ latent_diffusion.py     # U-Net V2 + all encoders
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                       # Data v1 (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ music_dataset.py        # PyTorch Dataset
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ training_dataset.json   # Dataset v1
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data_v2/                    # üÜï Data v2
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ segmented_dataset.py    # SegmentedMusicDataset
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ *.json                  # Datasets v2
‚îÇ
‚îú‚îÄ‚îÄ üìÅ music/
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ fma_small/              # FMA audio files
‚îÇ
‚îú‚îÄ‚îÄ üìÅ checkpoints/                # Checkpoints v1
‚îú‚îÄ‚îÄ üìÅ checkpoints_v2/             # üÜï Checkpoints v2
‚îÇ
‚îî‚îÄ‚îÄ üìÅ output/                     # Generated audio
```

---

## Usage Scenarios

### Scenario 1: Training from Scratch on Your Own MP3s

**When to use:** You have your own MP3 collection and want to train a model from scratch.

#### Step 1: Prepare Folder Structure

```bash
mkdir -p my_music/artist_name
cp ~/Music/*.mp3 my_music/artist_name/
```

#### Step 2: Generate Dataset

```bash
# Full pipeline with audio analysis, vocals and voice embeddings
python build_dataset_v2.py \
    --audio_dir ./my_music \
    --output ./data_v2/my_dataset.json \
    --device cuda \
    --batch_size 4
```

**Generated files:**
- `my_dataset.json` - metadata + prompts + all audio features (CLAP, voice, F0, etc.)
- `my_dataset.artist_embeddings.json` - average voice embeddings per artist

#### Step 3: Train VAE (Phase 1)

```bash
python train_v2.py \
    --phase 1 \
    --annotations ./data_v2/my_dataset.json \
    --audio_dir ./my_music \
    --epochs 50 \
    --batch_size 4 \
    --device cuda
```

**Time:** ~2-4h for 1000 tracks (GPU RTX 3090)

#### Step 4: Train Diffusion (Phase 3)

```bash
python train_v2.py \
    --phase 3 \
    --annotations ./data_v2/my_dataset.json \
    --audio_dir ./my_music \
    --vae_checkpoint ./checkpoints/vae_epoch_50.pt \
    --epochs 100 \
    --batch_size 2 \
    --device cuda
```

**Time:** ~8-12h for 1000 tracks (GPU RTX 3090)

---

### Scenario 2: Training on FMA Dataset

**When to use:** You have the FMA dataset and want to train a model.

#### Step 1: Download FMA (if you don't have it)

```bash
# FMA Small (~8GB, 8000 tracks)
wget https://os.unil.cloud.switch.ch/fma/fma_small.zip
unzip fma_small.zip -d ./music/
```

#### Step 2: Build Dataset v2

```bash
python build_dataset_v2.py \
    --audio_dir ./music/fma_small \
    --output ./data_v2/fma_dataset.json \
    --device cuda \
    --batch_size 4
```

**What build_dataset_v2 generates:**
| Field | Description | Source |
|-------|-------------|--------|
| `has_vocals` | Whether track has vocals | Whisper |
| `lyrics` | Text transcription | Whisper |
| `voice_embedding` | 256-dim vector | Resemblyzer |
| `ecapa_embedding` | 192-dim vector | ECAPA-TDNN |
| `clap_audio_embedding` | 512-dim | CLAP |
| `clap_text_embedding` | 512-dim | CLAP |
| `f0_contour` | Pitch contour | CREPE/pYIN |
| `vibrato_*` | Vibrato features | Custom |
| `breath_positions` | Breath timings | Custom |
| `phoneme_timestamps` | IPA + timing | Gruut/eSpeak |

#### Step 3: Training

```bash
# Phase 1: VAE
python train_v2.py --phase 1 \
    --annotations ./data_v2/fma_dataset.json \
    --audio_dir ./music/fma_small \
    --epochs 50

# Phase 3: LDM with voice conditioning
python train_v2.py --phase 3 \
    --annotations ./data_v2/fma_dataset.json \
    --audio_dir ./music/fma_small \
    --vae_checkpoint ./checkpoints_v2/vae_best.pt \
    --epochs 100
```

---

### Scenario 3: Adding New Tracks to Dataset

**When to use:** You already have a dataset and want to add new tracks.

#### Method A: Rebuild with New Folder

```bash
# Add new MP3s to folder
cp ~/new_music/*.mp3 ./music/fma_small/new/

# Rebuild dataset (will detect new files)
python build_dataset_v2.py \
    --audio_dir ./music/fma_small \
    --output ./data_v2/dataset_updated.json \
    --device cuda
```

#### Method B: Merge JSON

```python
import json

# Load existing
with open('data_v2/dataset.json') as f:
    dataset = json.load(f)

# Load new
with open('data_v2/new_tracks.json') as f:
    new_tracks = json.load(f)

# Merge (check duplicates by audio_path)
existing_paths = {t['audio_path'] for t in dataset}
for track in new_tracks:
    if track['audio_path'] not in existing_paths:
        dataset.append(track)

# Save
with open('data_v2/dataset_merged.json', 'w') as f:
    json.dump(dataset, f, indent=2)
```

#### Method C: Continue Training (Fine-tuning)

```bash
# Fine-tune on new data
python train_v2.py --phase 3 \
    --annotations ./data_v2/dataset_merged.json \
    --audio_dir ./music \
    --vae_checkpoint ./checkpoints_v2/vae_best.pt \
    --ldm_checkpoint ./checkpoints_v2/ldm_epoch_100.pt \
    --epochs 20  # Fewer epochs for fine-tuning
```

---

## Inference - Music Generation

### Basic Generation

```bash
python inference_v2.py \
    --prompt "Energetic electronic dance track with heavy bass" \
    --output ./output/edm_track.wav \
    --duration 30 \
    --device cuda
```

### With Artist Style (Voice Embedding)

```bash
python inference_v2.py \
    --prompt "Melodic hip-hop beat" \
    --style_of "Artist Name" \
    --output ./output/artist_style.wav
```

### With Voice Cloning

```bash
python inference_v2.py \
    --prompt "Upbeat pop song" \
    --voice_clone "Artist Name" \
    --lyrics "Here are the lyrics to sing..." \
    --output ./output/cloned_voice.wav
```

### With Structure Template

```bash
python inference_v2.py \
    --prompt "Energetic pop with female vocals" \
    --template verse_chorus \
    --duration 120 \
    --output ./output/structured_song.wav
```

### All Options

```bash
python inference_v2.py --help

# Main options:
#   --prompt TEXT          Prompt describing the music
#   --output PATH          Output path (default: ./output/generated.wav)
#   --duration FLOAT       Duration in seconds (default: 30)
#   --cfg_scale FLOAT      Classifier-free guidance (default: 7.5)
#   --num_steps INT        Denoising steps (default: 50)
#   --template NAME        Structure template (verse_chorus, etc.)
#
# Voice conditioning:
#   --style_of NAME/PATH   Artist voice embedding or .wav file
#
# Voice cloning:
#   --voice_clone NAME     Artist to clone voice from
#   --voice_clone_samples PATH  Folder/file with voice samples
#   --lyrics TEXT          Text to sing
#   --language CODE        Language code (pl, en, de, etc.)
```

---

## Detailed File Descriptions

### üìÑ `train_v2.py`

**Purpose:** Main v2 training script for VAE, Composition Planner and LDM.

**Training phases:**
1. **Phase 1 (VAE):** Audio ‚Üí Mel ‚Üí Latent ‚Üí Mel (reconstruction)
2. **Phase 2 (Composition Planner):** Track features ‚Üí Composition plan
3. **Phase 3 (LDM):** Noise ‚Üí UNet V2 (conditioned) ‚Üí Latent ‚Üí VAE ‚Üí Audio

**Key parameters:**
```python
# VAE
latent_dim = 128      # v2: increased from 8
sample_rate = 32000   # v2: 32kHz

# LDM
cfg_dropout = 0.1     # Classifier-free guidance dropout
voice_dropout = 0.1   # Voice conditioning dropout
```

---

### üìÑ `inference_v2.py`

**Purpose:** Generate music from trained v2 model.

**Main functions:**
- `generate_composition_plan()` - plan track structure
- `generate_section_audio()` - generate single section
- `generate_full_song()` - generate full track section by section

**Pipeline:**
1. Prompt ‚Üí T5/CLAP Encoder ‚Üí text embedding
2. (optional) Voice sample ‚Üí Resemblyzer/ECAPA ‚Üí voice embedding
3. (optional) Lyrics ‚Üí Gruut/eSpeak ‚Üí phonemes IPA
4. Template ‚Üí CompositionPlanner ‚Üí section structure
5. Per section: Noise + embeddings ‚Üí UNet V2 denoising ‚Üí Latent
6. Latent ‚Üí VAE Decoder ‚Üí Mel spectrogram
7. Mel ‚Üí Vocos ‚Üí Audio WAV
8. Concat all sections ‚Üí Final audio

---

### üìÑ `build_dataset_v2.py`

**Purpose:** Full feature extraction from audio files.

**Extracts:**
- Metadata (ID3 tags)
- Audio features (librosa: tempo, key, energy, etc.)
- Voice embeddings (Resemblyzer 256-dim + ECAPA-TDNN 192-dim)
- CLAP embeddings (audio 512-dim + text 512-dim)
- Pitch/F0 (CREPE/pYIN)
- Vibrato, breath, phoneme features
- Segment detection (verse/chorus/bridge)
- Lyrics transcription (Whisper)

**Output:** JSON with v3.1 fields (see DATASET diagram above)

---

### üìÑ `models_v2/latent_diffusion.py`

**Purpose:** UNet V2 + all conditioning modules.

**Main classes:**
- `UNetV2` - main diffusion model
- `SectionConditioningModule` - fusion of all conditioning
- `VoiceStreamAttention` - gated cross-attention for voice
- `VoiceEmbeddingFusion` - Resemblyzer + ECAPA fusion
- `PitchEncoder`, `VibratoEncoder`, `BreathEncoder` - feature encoders
- `BeatEmbedding`, `ChordEmbedding`, `PhonemeEncoder` - v2 encoders

---

### üìÑ `models/audio_vae.py`

**Purpose:** Audio compression to latent space.

**Architecture v2:**
```
Mel [1, 128, T] ‚Üí Encoder ‚Üí Œº, œÉ ‚Üí z [128, H, W] ‚Üí Decoder ‚Üí Mel [1, 128, T]
```

**Parameters:**
- `latent_dim = 8` - latent channel dimension
- `channels = [64, 128, 256, 512]` - encoder channels
- `n_mels = 128` - number of mel filterbanks (v2: increased from 80)

**Loss:**
```python
loss = reconstruction_loss + beta * kl_divergence + stft_loss
```

---

### üìÑ `models/text_encoder.py`

**Purpose:** Text prompt encoding.

**Backends:**
- `T5TextEncoder` - Flan-T5 (768-dim, good for long descriptions)
- `CLAPTextEncoder` - CLAP (specifically trained on audio-text)

---

### üìÑ `models/voice_synthesis.py`

**Purpose:** Voice cloning and synthesis.

**Usage:**
```python
# 1. Extract vocals
extractor = VoiceExtractorFromSong()
vocals_path = extractor.extract_vocals("song.mp3")

# 2. Register voice
synth = VoiceSynthesizer(backend="coqui")
synth.register_voice("artist", vocals_path)

# 3. Synthesize new text
audio = synth.synthesize("New lyrics...", voice="artist")
```

---

### üìÑ `data/music_dataset.py`

**Purpose:** PyTorch Dataset for training.

**Returns batch:**
```python
{
    'audio': torch.Tensor,           # [num_samples]
    'prompt': str,                   # "Energetic rock song..."
    'voice_embedding': torch.Tensor, # [256] or None
    'lyrics': str,                   # "Transcribed lyrics..."
    'has_vocals': bool,
    'text_sentiment': str,           # "positive"
    'track_id': int,
    'artist': str,
}
```

**Custom collate_fn:**
- Stacks tensors
- Groups strings into lists
- Handles None in voice_embedding

---

## FAQ & Troubleshooting

### ‚ùì Why `/var/folders/...` in vocals path?

**Question:** `Vocals saved to: /var/folders/fg/frwh54994k9gy6h5y_tc1_940000gn/T/2_Food_vocals.wav`

**Answer:** This is the **default macOS temporary folder** (`tempfile.gettempdir()`).

`VoiceExtractorFromSong` saves extracted vocals to the system temporary folder by default, which on macOS is:
```
/var/folders/XX/XXXX/T/
```

**Solution:** Set your own `output_dir`:

```python
extractor = VoiceExtractorFromSong(
    output_dir="./data/separated_vocals"  # Permanent folder
)
```

Or when building dataset with `build_dataset_v2.py` use `--separate_vocals` flag.

---

### ‚ùì Training is Very Slow on CPU

**Problem:** Training on CPU takes hours even for a few tracks.

**Solutions:**
1. Use GPU: `--device cuda`
2. Reduce batch size: `--batch_size 1`
3. Reduce number of tracks: `--max_tracks 10`
4. Use mixed precision (auto on GPU)

---

### ‚ùì `CUDA out of memory`

**Problem:** GPU doesn't have enough memory.

**Solutions:**
1. Reduce batch size: `--batch_size 1`
2. Use gradient checkpointing (enabled by default)
3. Use smaller VAE model
4. Shorten duration: change in code `duration=5.0`

---

### ‚ùì Voice Cloning Sounds Robotic

**Problem:** XTTS generates artificial voice.

**Solutions:**
1. Use longer voice sample (>30s)
2. Make sure sample has clean vocals (no instruments)
3. Use ElevenLabs instead of Coqui (better quality, paid)

---

### ‚ùì Whisper Doesn't Detect Vocals

**Problem:** `has_vocals: false` for tracks with vocals.

**Causes:**
1. Instrumental too loud
2. Vocals in unsupported language
3. Analyzed fragment too short

**Solutions:**
1. Use `--whisper_full` (analyze entire track)
2. Use larger model: `--whisper_model medium`
3. First separate vocals: `--separate_vocals`

---

### ‚ùì Missing Module `speechbrain`

**Warning:** `No module named 'speechbrain'`

**Solution:** System automatically uses `resemblyzer` as fallback. If you want SpeechBrain:
```bash
pip install speechbrain
```

---

## üìä Model Size Configuration

### Model Size Parameters

| Parameter | Impact | Description |
|-----------|--------|-------------|
| `latent_dim` | Minimal (~3M) | VAE latent space dimension |
| `model_channels` | **KEY** | Base UNet channel width - main "size knob" |

### Model Size Table

| Config | latent_dim | model_channels | VAE | UNet | **Total** |
|--------|-----------|----------------|-----|------|-----------|
| Test/Dev | 128 | 256 | 224M | 722M | **~1B** |
| Production Default | 128 | 320 | 224M | 1.1B | **~1.3B** |
| Large Production | 128 | 512 | 224M | 2.8B | **~3B** |
| XL Production | 256 | 512 | 228M | 2.8B | **~3B** |
| XXL (multi-billion) | 256 | 768 | 228M | 6.1B | **~6.4B** |

### Conclusions

- **`latent_dim=128` is sufficient** - difference between 128 and 256 is only ~3M parameters in VAE (~1.5% difference)
- **`model_channels` is the real "size knob"** - increasing from 320‚Üí512 gives jump from 1.1B‚Üí2.8B
- For **several billion parameters**: `model_channels=512-768` is key

### Recommendations

| Use Case | Configuration | Size |
|----------|---------------|------|
| Local testing/dev | `latent_dim=128, model_channels=256` | ~1B |
| Standard production | `latent_dim=128, model_channels=320` | ~1.3B |
| Large production model | `latent_dim=128, model_channels=512` | ~3B |
| Very large model | `latent_dim=256, model_channels=768` | ~6.4B |

### Code Configuration Example

```python
# Test/Dev (~1B)
unet = UNetV2(
    in_channels=128,
    out_channels=128,
    model_channels=256,  # smaller for quick testing
    context_dim=768,
)

# Production (~3B)
unet = UNetV2(
    in_channels=128,
    out_channels=128,
    model_channels=512,  # larger for quality
    context_dim=768,
)
```

### AudioVAE - Full Configuration

**`AudioVAE.__init__` parameters:**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `sample_rate` | 32000 | v2: 32kHz (v1: 22050) |
| `n_mels` | 128 | Number of mel bins |
| `n_fft` | 1024 | FFT window size |
| `hop_length` | 320 | 10ms hop @ 32kHz |
| `latent_dim` | 128 | v2: increased from 8 |
| `channels` | None | Auto-select from `LATENT_CONFIGS` |
| `use_stft_loss` | True | Multi-Resolution STFT Loss |
| `use_checkpoint` | False | Gradient checkpointing (saves VRAM) |

**Auto-select channels (`LATENT_CONFIGS`):**

| latent_dim | channels (auto) | VAE Size |
|------------|-----------------|----------|
| 8 | [64, 128, 256, 512] | **55M** |
| 32 | [64, 128, 256, 512] | **56M** |
| 64 | [96, 192, 384, 768] | **125M** |
| 128 | [128, 256, 512, 1024] | **224M** |

**Custom channels - full scale:**

| Config | channels | Size |
|--------|----------|------|
| v2 Light | [64, 128, 256, 512] | **57M** |
| v2 Default | [128, 256, 512, 1024] | **224M** |
| v2 Heavy | [256, 512, 1024, 2048] | **889M** |

**VAE configuration examples:**

```python
# Default v2 (224M) - recommended
vae = AudioVAE(latent_dim=128)

# Light (57M) - quick tests
vae = AudioVAE(latent_dim=128, channels=[64, 128, 256, 512])

# Heavy (889M) - maximum reconstruction quality
vae = AudioVAE(latent_dim=128, channels=[256, 512, 1024, 2048])

# With gradient checkpointing (less VRAM)
vae = AudioVAE(latent_dim=128, use_checkpoint=True)
```

---

## Requirements

```txt
# Core
torch>=2.0
torchaudio>=2.0
transformers>=4.30
einops
vocos

# Audio processing
librosa
soundfile
mutagen

# Whisper (optional)
faster-whisper  # or openai-whisper

# Voice embeddings (one of):
resemblyzer        # lightweight (256-dim)
speechbrain        # better (192-dim ECAPA-TDNN)

# Voice cloning (optional)
TTS                # Coqui XTTS v2
demucs             # Vocal separation

# LLM (optional)
openai             # GPT-4
requests           # Ollama
```

---

## License

GPL-2.0 License - use for your own projects!

‚ö†Ô∏è **Legal notice:** Voice cloning may violate artists' voice likeness rights. Use only with your own voice or with the owner's consent.

---

## Related Documents

- üìò [Dataset Builder - Full Documentation](docs_v2/DATASET_BUILDER.md)

---

*Documentation generated: December 14, 2025*

